Store the kafka  file in c drive only. if we try to store in the other file it  will give the error that file path is too long when we try to strt zookeper server.
C:\kafka\kafka\bin\windows>zookeeper-server-start.bat ..\..\config\zookeeper.properties------> to start zookeeper server
C:\kafka\kafka\bin\windows>kafka-server-start.bat ..\..\config\server.properties---->Start Kafka Server / Broker
************************************************************************************************************************
C:\kafka\kafka\bin\windows>kafka-server-stop.bat-->to stop kafka server.. use this command on the other termain other than the terminal
in which kafka server is running
To create topic
C:\kafka\kafka\bin\windows>kafka-topics.bat --create --topic java --bootstrap-server localhost:9092 --replication-factor 1 --partitio
ns 3
Created topic java.
*************************************************************************************************************************************************************************
Describe topics
C:\kafka\kafka\bin\windows>kafka-topics.bat --bootstrap-server localhost:9092 --describe --topic java
Topic: java     TopicId: gI2J47QdT2qEksslN9kxvQ PartitionCount: 3       ReplicationFactor: 1    Configs:
        Topic: java     Partition: 0    Leader: 0       Replicas: 0     Isr: 0
        Topic: java     Partition: 1    Leader: 0       Replicas: 0     Isr: 0
        Topic: java     Partition: 2    Leader: 0       Replicas: 0     Isr: 0
***********************************************************************************************************************************************************************
C:\kafka\kafka\bin\windows>kafka-console-producer.bat --broker-list localhost:9092 --topic java-->to start the producer
**********************************************************************************************************
kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic java --from-beginning-->to start the consumer
**************************************************************************************************************************
C:\kafka\kafka\bin\windows>kafka-consumer-groups.bat --bootstrap-server localhost:9092 --list-->to see how many consumer group 
are present
*************************************************************************************************************************
C:\kafka\kafka\bin\windows>kafka-consumer-groups.bat --bootstrap-server localhost:9092 --describe --group javaConsmuer-group-3
it describe the particular consumer group "javaConsmuer-group-3"-->this is the consumer group
**************************************************************************************************************
C:\kafka\kafka\bin\windows>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic java --group myConsumerGroup
given the group name for the consumer

***************************************************************************************************************************


C:\kafka\kafka\bin\windows>kafka-topics.bat --create --topic java4 --bootstrap-server localhost:9092, localhost:9093, localhost:9094 --replication-factor 3 --partitions 3
It will create one topic with three copy . In my case there are three brocker. These brocker will have a copy of the data
******************************************************************************************************************
Multiple producer can happen for same topic
******************

Topics-->
A stream of message belonging to particular category is called a topic.
It is logical feed name to whch records are published.
It is Simlar to a table in a database.(records are considred message here)
Unique identifier of topic is it name.
we can create as many topics as we want.
we can create the topic for finance->this topics contain finance related messagge just like finance table
sport topic
******************************************************************************************************************
kafka cluster-->Kafka Broker-->kafka topics-->partition
*********************************************************************
Topics are split into partions. All the message within the partion are ordered and immutable. Each message within a partion 
has unique id known as offset
**********************
Replicas are backup of partion. replicas are never read or write data. they are used to prevent data loss.
********************************
producer are application that produces message to the topics within a cluster using the producing Apis. they produce message at topic level or partion level
****************************************************************************************************
Consumers are always associated with exactly one consumer group.
A consumer group is a group of related consumer that perform a task.
consumer are application tht read data from topics
*****************************************************************
Brokers are simple software processes who maintain and manage the published message.
Also known as kafka servers.
Brokers also manages the conumer-offsets and are responsible for the delivery of message to the right consumers.
A set of brokers who are communicating with each other to perform the management and maintanence task are collectively known as
kafka cluster.
we can add more broker in a alread runnning kafka cluster without any downtime
****************************
Zookeeper-->
it is used to moniotr kafka cluster and cordinate with each broker
keeps all the metadata informaion related to kafka cluster in the form of key value pair.
Metadata include -->Condfiguration information. Health status of each broker. 
********************************************************************************************************************
Kafka is 
Sclable->Horizontal Scaling is done by adding new brokers to the existing cluster.
Fault tolerence->Kafka cluster can handle failures because of its distributed nature.
Durable-->Kafka uses "Distributed commit log" which means message persist on a disk as fast as possible.
Zero down time-->It ensure zero downtime when required number of brokers are present in the cluster
**************************************************************
It is strongly recomended to run odd no of zookeper cluster
***************************************************************************
Leader will be decided by the partition. suppose there are three partion and two brocker then brocker 1 may be the leader partion 1 and 3 
and brocker 2 may be the leader of partion 2.
**************************************************************************************
The replicated group of server is called quoroum.zookeeper server run as leader folower node. If leader fails a new leader 
is elected.
************************************************************************************
Kafka cluster is managed by Zookeeper clutster(Zookeeper cluster have zookeeper node)
***********************************
Never no of replication factor should be greater than the no of avilable brocker.Brocker is nothing but a kafka server.
***********************************************************************
No of partition we always can increase but we never decrease the no of the partition.
*****************************************************************************************
we can move the partition from one brocker to another broker 
****************************************************
we can increase the replication factor
***********************************************************************************************************
we can change the topic partition
**********************************************************************************************************
we can increase the partition 
****************************************************************************************************
Group leader--> In Kafka, when several consumers come together to form a group (a consumer group) to read data, 
the first consumer that connects is automatically chosen as the group leader.
This leader keeps track of all the other consumers that are currently connected as part of the group
 Whenever a new consumer joins the group or an existing one leaves, the group leader organizes a rebalance. This means the leader reassesses and redistributes the tasks 
(which parts of the data each consumer should handle) among all current members.
*************************************************************************************************
Group cordinator-->In a Kafka system, which is used to handle and store real-time data streams, there are servers known as brokers. 
Some of these brokers are given a special role called "group coordinators."
When a consumer (a program that reads and processes data) starts, it first needs to find out who the manager (coordinator) is for its group. 
It does this by sending a "FindCoordinator" request. 
Once the consumer knows who the coordinator is, it tells the coordinator that 
it wants to join the group by sending a "Join Group" request.While the consumer is working, it regularly sends a "Heartbeat" request to the coordinator. 
This lets the coordinator know that the consumer is still active and working.
****************************************************************************************************
5. During the time the rebalancing process is happening,
 the consumers in that group can't read any messages. They need to wait until the rebalance is complete and they have 
their new instructions.
*************************************
offset-->In Kafka, each message or record in a partition is given a unique number called an "offset". 
This number helps identify each record uniquely within its partition. 
***********************************
Log-end Offset:
This is the offset of the newest record added to a partition.
The log-end offset tells you what the current last page (or record number) is.
************
Current Offset:
This offset points to the last record that Kafka has sent to a consumer the last time it asked for data.
***************************************
Committed Offset:
Simple Explanation: When you're consuming records (like reading pages in a book), 
marking an offset as committed is like telling the system, "I've read up to here."
************************************************
Internals when producer send a message-->
Message:-key:
payload:"abcd"
topic
partition:
offset:
timestamp:
************************************************************
consumer.poll method is used by Kafka consumers to fetch data from the broker. 
When you call this method, the consumer reaches out to the Kafka broker and requests any new records that are available since the last poll.
Request for Records: You specify a timeout when calling consumer.poll(timeout). The consumer waits for the specified duration to receive any new records from the server. The duration can be set based on how responsive you want your consumer to be.
Batch of Records: The method returns a list of records that can be processed by the consumer. These records are grouped by the partitions they came from, allowing efficient processing.
Handling Data: Once records are fetched, your application can process them accordingly.
**********************************
consumer.commit method is used to manually commit the offsets of records that have been processed.
 When you commit an offset, you are telling Kafka that the consumer has successfully processed all prior records up to that offset. 
The committed offset is used as a checkpoint from which the consumer can resume reading in case of a failure or restart.
***************************
Types of Commit:
Synchronous Commit: You can commit offsets synchronously using consumer.commitSync().
This method will block until the broker confirms the commit, ensuring reliability at the cost of latency and throughput.
Asynchronous Commit: You can also commit offsets asynchronously using consumer.commitAsync(). 
This method does not block; it sends the offsets to be committed and immediately returns to allow further processing.
*****************************************************
consumer.poll is about fetching records from Kafka.
consumer.commit is about acknowledging the successful processing of those records.
******************************
When Does Rebalancing Happen?
A Consumer Joins the Group:
A Consumer Leaves the Group:
Partitions Added to a Topic:
A Partition Goes Offline:
****************************************
Rebalancing is crucial for distributing work evenly across a consumer group. Without rebalancing, some consumers might end up doing more work than others, 
or some partitions might not get processed at all if their assigned consumer fails.
**************************************
Serialization is the process of converting an object (which could be any kind of data structure,
 like a list, a dictionary, or a custom object) into a stream of bytes. This conversion makes 
it easy to store the object (for example, in a file or a database) or send it over a network to another computer.
********************
Responsibilities of the Group Coordinator:
Handles Rebalancing of Consumers:
Blocks Data Reads During Rebalancing:
********************************************************************************************
connections.max.idle.ms-->
This setting determines the maximum amount of time (in milliseconds)
 that a network connection can remain idle (unused) before Kafka decides to close it.
An idle connection is one where no data is being sent or received.
*****************************************************************************************
ACK=0
 When you set acks to 0, the producer sends the data but does not wait for any confirmation 
from the server that the data has been received.
********************
ACK=1
With acks set to 1, the producer waits for a basic acknowledgment from the leader (the main server handling that piece of data). The leader confirms it has saved the data,
 but it doesn't wait for all the follower servers to save it too.
********************
Acks=-1 or Acks=all
What It Means: This setting requires that the leader and all in-sync follower servers confirm that they have received the data.
******************
compression.type is used by producers to compress the messages they send. 
The compression.type configuration specifies how messages should be compressed before they're sent by the producer to the Kafka server. Compression helps reduce the size of the messages, 
which can save network bandwidth and storage space on the Kafka brokers.
******************
Valid Values:
none: No compression is applied.
gzip: Uses Gzip for compression, which is effective but might be slightly slower in terms of compression and decompression speed.
snappy: Provides a good balance between compression rate and speed, making it faster at compressing and decompressing.
lz4: Similar to Snappy, it's fast but offers a slightly better compression ratio compared to Snappy.
zstd: This is a newer compression algorithm that offers excellent compression ratios at decent speeds.
*************************************************************
Batching Impact: Kafka compresses data in batches. 
This means that the effectiveness of compression can be influenced by how much data you batch together. 
Larger batches typically can be compressed more effectively compared to smaller ones,
 because the compression algorithm works better with more data to analyze.
***************************
batch.size setting is an important configuration for producers (the components that send data to Kafka). 
It determines how data is grouped and sent to the Kafka brokers. 
The batch.size value specifies the maximum amount of data (in bytes) that can be included in one batch. If you have lots of small messages going to the same partition, 
they can be grouped together up to this size limit.
****************************************
Default: The default value of batch.size is 16384 bytes (which is about 16KB). 
This is a balanced choice for typical scenarios.
**************************************************************************************
Buffer Memory: The buffer.memory specifies the total amount of memory (in bytes) that the producer is allowed to use for storing records that are waiting to be sent to the Kafka broker. 
This acts as a holding area for data before it is successfully delivered.
**************************************************************************************
max.block.ms setting determines the maximum amount of time (in milliseconds) that the Kafka producer will wait when trying to send data and the buffer is full. 
This is the maximum time the producer is blocked before it decides it cannot send data.
The linger.ms setting controls how long the Kafka producer waits, hoping that more records will
 arrive to fill up the same batch before sending the batch to the server.
******************************************************************

